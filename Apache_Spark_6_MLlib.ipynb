{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "4Ho4DEGs2Ixu",
        "outputId": "41f834d3-b2ad-41bf-af39-b1a8f06eae4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=e26b9f109fd3073ec73c91139543aee05ce3e802758422d95f6782c64109d8df\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 121752 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u392-ga-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u392-ga-1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u392-ga-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u392-ga-1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u392-ga-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u392-ga-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import pyspark.sql  as pyspark_sql\n",
        "import pyspark.sql.types as pyspark_types\n",
        "import pyspark.sql.functions  as F\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql.functions import row_number, desc\n",
        "\n",
        "\n",
        "\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = pyspark_sql.SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "r_IExLdN2OFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "RYfwy6-n3K0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of data as tuples\n",
        "data = [(-1, -1, 1), (-2, -1, 1), (-3, -2, 1), (1, 1, 2), (2, 1, 2), (3, 2, 2)]\n",
        "\n",
        "# Create a DataFrame from the list of tuples\n",
        "df = spark.createDataFrame(data, [\"feature1\", \"feature2\", \"label\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "oLAYW5_s1ufQ",
        "outputId": "f280bb6a-726b-4fbd-a08b-33f0b9d87f71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+-----+\n",
            "|feature1|feature2|label|\n",
            "+--------+--------+-----+\n",
            "|      -1|      -1|    1|\n",
            "|      -2|      -1|    1|\n",
            "|      -3|      -2|    1|\n",
            "|       1|       1|    2|\n",
            "|       2|       1|    2|\n",
            "|       3|       2|    2|\n",
            "+--------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert label (string) to integer for classification\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(df)\n",
        "df = labelIndexer.transform(df)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "Ss3G-mrY2kbg",
        "outputId": "d30f4c92-9e74-4f96-a193-eeb65a2d5c3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+-----+------------+\n",
            "|feature1|feature2|label|indexedLabel|\n",
            "+--------+--------+-----+------------+\n",
            "|      -1|      -1|    1|         0.0|\n",
            "|      -2|      -1|    1|         0.0|\n",
            "|      -3|      -2|    1|         0.0|\n",
            "|       1|       1|    2|         1.0|\n",
            "|       2|       1|    2|         1.0|\n",
            "|       3|       2|    2|         1.0|\n",
            "+--------+--------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n",
        "\n",
        "df = assembler.transform(df)\n",
        "(trainingData, testData) = df.randomSplit([0.5, 0.5])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "_Or2BZJ92q7_",
        "outputId": "f5194e1a-942d-4885-ae1a-64e505d0cc04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+-----+------------+-----------+\n",
            "|feature1|feature2|label|indexedLabel|   features|\n",
            "+--------+--------+-----+------------+-----------+\n",
            "|      -1|      -1|    1|         0.0|[-1.0,-1.0]|\n",
            "|      -2|      -1|    1|         0.0|[-2.0,-1.0]|\n",
            "|      -3|      -2|    1|         0.0|[-3.0,-2.0]|\n",
            "|       1|       1|    2|         1.0|  [1.0,1.0]|\n",
            "|       2|       1|    2|         1.0|  [2.0,1.0]|\n",
            "|       3|       2|    2|         1.0|  [3.0,2.0]|\n",
            "+--------+--------+-----+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DecisionTreeClassifier model\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# Train the model on the training data\n",
        "model = dt.fit(trainingData)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Evaluate the model performance (optional)\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "kRSIQSA72-_w",
        "outputId": "1e297750-98d7-48e0-9197-092060733635",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "bP2jzjnx3NhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"LinearRegressionExample\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "rfVSRsEQ5Jud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y = 1 * x_0 + 2 * x_1 + 3\n",
        "\n",
        "# Create a list of data as tuples\n",
        "data = [(1, 1, 6), (1, 2, 8), (2, 2, 9), (2, 3, 11)]"
      ],
      "metadata": {
        "id": "mVRU7PBJ5NIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from the list of tuples with column names\n",
        "df = spark.createDataFrame(data, [\"x1\", \"x2\", \"y\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "KdWaNmlF5qnN",
        "outputId": "d4022f9c-c663-4ed5-c77a-a6bdfe288d44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+\n",
            "| x1| x2|  y|\n",
            "+---+---+---+\n",
            "|  1|  1|  6|\n",
            "|  1|  2|  8|\n",
            "|  2|  2|  9|\n",
            "|  2|  3| 11|\n",
            "+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble features into a single vector\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=[\"x1\", \"x2\"], outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "df = df.withColumnRenamed(\"y\", \"label\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "7jQjwL5C5t0v",
        "outputId": "e83e7887-b26c-47a7-f16a-8ae8274eb99a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-----+---------+\n",
            "| x1| x2|label| features|\n",
            "+---+---+-----+---------+\n",
            "|  1|  1|    6|[1.0,1.0]|\n",
            "|  1|  2|    8|[1.0,2.0]|\n",
            "|  2|  2|    9|[2.0,2.0]|\n",
            "|  2|  3|   11|[2.0,3.0]|\n",
            "+---+---+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a LinearRegression model\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Train the model on the data\n",
        "model = lr.fit(df)\n"
      ],
      "metadata": {
        "id": "UZu-wmPU44Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on new data\n",
        "newData = spark.createDataFrame([(3, 3)], [\"x1\", \"x2\"])\n",
        "newData = assembler.transform(newData)\n",
        "predictions = model.transform(newData)\n",
        "predictions.select(\"features\", \"prediction\").show()\n",
        "\n",
        "# Print the model coefficients and intercept\n",
        "print(\"Coefficients:\", model.coefficients)\n",
        "print(\"Intercept:\", model.intercept)\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "QpDLVT-E6AjP",
        "outputId": "ab50859f-769c-445f-f208-2ed5f510b4ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------------+\n",
            "| features|        prediction|\n",
            "+---------+------------------+\n",
            "|[3.0,3.0]|12.000000000000004|\n",
            "+---------+------------------+\n",
            "\n",
            "Coefficients: [1.0000000000000033,2.0000000000000018]\n",
            "Intercept: 2.99999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering"
      ],
      "metadata": {
        "id": "hczs-Sxk6HJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"KMeansExample\").getOrCreate()\n",
        "\n",
        "# Create a list of data points\n",
        "data = [(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = spark.createDataFrame(data, [\"feature1\", \"feature2\"])\n",
        "\n",
        "# Assemble features into a single vector\n",
        "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n",
        "df = assembler.transform(df)"
      ],
      "metadata": {
        "id": "3Mymi1Y56iFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a KMeans model with 2 clusters\n",
        "kmeans = KMeans(k=2)\n",
        "\n",
        "# Train the model on the data\n",
        "model = kmeans.fit(df)\n",
        "\n",
        "# Make predictions on the data\n",
        "predictions = model.transform(df)\n",
        "\n",
        "# Print the predictions\n",
        "predictions.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "6nTwCSJf6k_A",
        "outputId": "27f09763-187d-480f-eb3b-566eaad969a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+----------+----------+\n",
            "|feature1|feature2|  features|prediction|\n",
            "+--------+--------+----------+----------+\n",
            "|       1|       2| [1.0,2.0]|         0|\n",
            "|       1|       4| [1.0,4.0]|         0|\n",
            "|       1|       0| [1.0,0.0]|         0|\n",
            "|      10|       2|[10.0,2.0]|         1|\n",
            "|      10|       4|[10.0,4.0]|         1|\n",
            "|      10|       0|[10.0,0.0]|         1|\n",
            "+--------+--------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Association Rule Mining"
      ],
      "metadata": {
        "id": "fkJsTKxN8RAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"LinearRegressionExample\").getOrCreate()"
      ],
      "metadata": {
        "id": "RmdxYrHV8ZJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "('Tom', ['Bread', 'Eggs', 'Milk']),\n",
        "('Dick', ['Bread', 'Eggs', 'Butter', 'Milk']),\n",
        "('Harry', ['Bread', 'Eggs'])\n",
        "], [\"name\", \"items\"])\n",
        "\n",
        "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.5, minConfidence=0.6)\n",
        "model = fpGrowth.fit(df)\n",
        "model.transform(df).show()\n",
        "\n",
        "test_df = spark.createDataFrame([\n",
        "('John', ['Bread', 'Eggs'])\n",
        "], [\"name\", \"items\"])\n",
        "\n",
        "model.transform(test_df).show()"
      ],
      "metadata": {
        "id": "2JpwyAJ48VV_",
        "outputId": "e1c1c501-b806-4252-dcfa-02c05edb2b2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+----------+\n",
            "| name|               items|prediction|\n",
            "+-----+--------------------+----------+\n",
            "|  Tom| [Bread, Eggs, Milk]|        []|\n",
            "| Dick|[Bread, Eggs, But...|        []|\n",
            "|Harry|       [Bread, Eggs]|    [Milk]|\n",
            "+-----+--------------------+----------+\n",
            "\n",
            "+----+-------------+----------+\n",
            "|name|        items|prediction|\n",
            "+----+-------------+----------+\n",
            "|John|[Bread, Eggs]|    [Milk]|\n",
            "+----+-------------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}